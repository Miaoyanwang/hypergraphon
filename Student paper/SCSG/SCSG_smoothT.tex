\documentclass{article}
\newcommand{\blind}{1}
% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[preprint]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
     %\usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{amsmath,amssymb,dsfont,color,bm,mathtools,enumitem}
\mathtoolsset{showonlyrefs}
\usepackage{amsthm}
\usepackage{subcaption}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}
\input macros.tex
\usepackage{mathrsfs}  
\def\caliB{\mathscr{B}}


\setlength{\bibsep}{0.0pt}

\begin{document}

\if1\blind
{   \title{Smooth tensor estimation with \\
unknown permutations}
\author{%
  Chanwoo Lee \\
  University of Wisconsin-Madison\\
  \texttt{chanwoo.lee@wisc.edu} \\
  % examples of more authors
   \And
   Miaoyan Wang \\
   University of Wisconsin-Madison \\
   \texttt{miaoyan.wang@wisc.edu} }

    \maketitle
} \fi

\if0\blind
{
 \date{}
  \title{Smooth tensor estimation with \\
unknown permutations}
\author{}
\maketitle
} \fi



\begin{abstract}
 We consider the problem of structured tensor denoising in the presence of unknown permutations. Such data problems arise commonly in recommendation system, community detection, and multiway comparison applications. Here, we develop a general family of smooth tensors up to arbitrarily index permutations; the model incorporates the popular block models and graphon models. We show that a constrained least-squares estimate in the block-wise polynomial family achieves the minimax error bound. A phase transition phenomenon is revealed with respect to the smoothness threshold needed for optimal recovery. In particular, we find that a polynomial of degree of $m(m-1)/2$ is sufficient for accurate recovery of order-$m$ tensors, whereas higher degree exhibits no further benefits.  Furthermore, we provide an efficient polynomial-time Borda count algorithm that provably achieves optimal rate under monotonicity assumptions. The efficacy of our procedure is demonstrated through both simulations and Chicago crime date analysis. 
\end{abstract}
\vspace{-.5cm}
\section{Introduction}\label{sec:int}
\vspace{-.4cm}
Higher-order tensor datasets are rising ubiquitously in modern data science applications.
Tensor structure provides effective representation of data that classical vector- and matrix-based methods fail to capture. 
One example is music recommendation system that records ratings of songs from users on different contexts \citep{baltrunas2011incarmusic}. This three-way tensor of user$\times$song$\times$context allows us to investigate interaction of users and songs under a context-specific manner.
Another example is network analysis that studies the connection pattern among nodes.  Pairwise interactions are often insufficient to capture the complex relationships, whereas multi-way interactions improve understanding the networks in molecular system \citep{Michoel2012AlignmentAI} and computer vision \citep{Agarwal2006HigherOL}. In both examples, higher-order tensors represent multi-way interactions in an efficient way.


%Tensor estimation problem cannot be solved without imposing structure. An appropriate reordering of tensor entries can provide effective representation of the hidden signal structure. For example, consider the  music recommendation system \citep{baltrunas2011incarmusic}. Suppose that we have a certain criterion available (such as similarities of music genres, age of users, and positive versus negative effect of  contexts) to reorder songs, users, and contexts with. Then, the sorted tensor has smooth structure because the entries from similar groups tend to have close values. Another example is hypergraph analysis. Hypergraph considers multi-way interactions of nodes and has hyperedges connecting more than two nodes. If we know the characteristics of individual nodes and rearrange them based on their similarities, the sorted adjacency tensor will have a special structure by the same reason.

\begin{figure}[h]
    \centering
    \includegraphics[width = .6\textwidth]{figures/semantic_new.pdf}
    \caption{(a): Illustration of order-$m$ $d$-dimensional permuted smooth tensor models with $m=2$. (b): Phase transition of mean squared error (MSE) (on $-\log d$ scale) as a function of smoothness
$\alpha$ and tensor order $m$. Bold dots correspond to the critical smoothness level above which higher
smoothness exhibits no further benefits to tensor estimation.}
    \label{fig:rate}
   \vspace{-.3cm}
\end{figure}

Tensor estimation problem cannot be solved without imposing structure. 
We study a class of structured tensors, \emph{permuted smooth tensors} of the following form:
%Let $\tY$ be an order-$m$ $d$-dimensional symmetric tensor generated from the model,
%\begin{align}\label{eq:gmd}
%    \tY = \Theta\circ \pi + \text{noise}, \quad \text{where}\quad \Theta_{i_1,\ldots,i_m}=f(i_1,\ldots,i_m),
%\end{align}
\begin{equation}\label{eq:rep}
\tY=\Theta\circ \pi+\tE,\quad\text{where}\quad \Theta_{i_1,\ldots,i_m} = f\left({i_1\over d},\ldots,{i_m\over d}\right).
\end{equation}
where $\pi\colon[d]\rightarrow[d]$ is an unknown latent permutation,  $\Theta\in \mathbb{R}^{d\times \cdots\times d}$ is an unknown symmetric signal tensor, and $f$ is an unknown multivariate function with smoothness index $\alpha>0$ (see Figure~\ref{fig:rate}(a)), and $\tE$ is a symmetric noise tensor consisting of zero-mean, independent sub-Gaussian entries with variance bounded by $\sigma^2$. For simplicity of presentation, we focus on symmetric tensors in the main paper; our models and techniques easily generalize to non-symmetric tensors.  Our primary goal is to estimate a permuted smooth signal tensor from a noisy observation. 

{\bf Related work and our contributions.} The estimation problem of~\eqref{eq:rep} falls into the general category of structured learning with \emph{latent permutation}, which has recently observed a surge of interest. Models involving latent permutations include graphon~\cite{gao2018minimax,klopp2017oracle}, stochastic transitivity models~\cite{shah2019low},  and crowd labeling~\cite{shah2020permutation}. Most of these methods are developed for matrices. The tensor counterparts  are far less well understood. 
%Table~\ref{tab:comp} summarizes the related works on tensor learning with latent permutations.
%\begin{table*}[http]
%    \centering
%    \resizebox{\textwidth}{!}{%
%    \begin{tabular}{c|cccc}
%    & Pananjady et al~\cite{pananjady2020isotonic}&  Balasubramanian~\cite{balasubramanian2021nonparametric}&  Li et al~\cite{li2019nearest}&\textbf{Ours$^*$}\\
%    \hline
%       model structure& monotonic & Lipschitz & Lipschitz &  $\alpha$-smoothness  \\
%     minimax lower bound& $\surd$  & $\times$ & $\times$ & $\surd$ \\
%     error rate for order-3 tensors& $d^{-1}$ & $d^{-6/5}$ & $d^{-1}$ & $d^{-2}$ \\
%     polynomial algorithm& $\surd$ &$\times$ & $\surd$ & $\surd$\\
%        \hline
%    \end{tabular}
%    }
%    \caption{\small Comparison of our results with previous works. $^*$We  list here only the result for infinitely smooth order-3 tensors. Our results allow general tensors of arbitrary order $m$ and smoothness $\alpha$; See Theorems~\ref{thm:LSE} and \ref{thm:BC}.}\label{tab:comp}
%\end{table*}
%    \vspace{-.4cm}

%and its statistical hardness in terms of minimax rate, and construct a polynomial-time algorithm for the estimation.
%We propose two estimating algorithms with accuracy guarantees: the least square estimation and Borda count estimation. 
We summarize our major contributions.
1). We develop a general permuted $\alpha$-smooth tensor model for an arbitrary smoothness index $\alpha>0$. In contrast to earlier work~\cite{balasubramanian2021nonparametric,li2019nearest} that focuses only on $\alpha=1$, we establish the statistically optimal error rate and its dependence on tensor order, dimension, and smoothness index. 
%We establish the upper bound of the least-square estimate with high probability and show that this upper bound matches with minimax lower bound implying its optimality. 
2). We discover an intriguing phase transition phenomenon with respect to the smoothness threshold needed for optimal tensor recovery in model~\eqref{eq:rep}. The critical threshold $\alpha^*$ (defined in Theorem~\ref{thm:LSE}) characterizes two distinct error dependence behaviors on the smooth index $\alpha$. We proved that the error decreases with $\alpha$ in the range $\alpha<\alpha^*$, whereas the error is a constant of $\alpha$ in the range $\alpha>\alpha^*$ (see Figure~\ref{fig:rate}(b)).
These results are distinct from the matrix counterparts~\citep{gao2016optimal,klopp2017oracle,gao2018minimax}, thereby highlighting the fundamental challenges with tensors. 
3).  We provide an efficient polynomial-time Borda count algorithm that provably achieves optimal rate under monotonicity assumptions. Numerical analysis demonstrates the competitive performance of our algorithm.



{\bf Notation.} We use $[d]=\{1,\ldots,d\}$ for $d$-set with $d\in\mathbb{N}_{+}$. For a set $S$, $\mathds{1}_S$ denotes the indicator function. For positive two sequences $\{a_n\},\{b_n\}$,  we denote $a_n\lesssim b_n$ if $\lim_{n\to\infty} a_n/b_n\leq c$,  and $a_n\asymp b_n$ if $c_1\leq \lim_{n\to \infty} a_n/b_n\leq c_2$ for some constants $c,c_1,c_2>0$. Given number $a\in\mathbb{R}$, the floor function $\lfloor a\rfloor$ is the largest integer no greater than $a$, and the ceiling function $\lceil a\rceil$ is the smallest integer no less than $a$.
We use $\tO(\cdot)$ to denote the big-O notation.  An event $A$ is said to occur \emph{with high probability} if $\mathbb{P}(A)$ tends to 1 as the tensor dimension $d\to\infty$.We use $\Theta_{i_1,\ldots,i_m}$ to denote the tensor entry indexed by $(i_1,\ldots,i_m)$, and use $\Theta\circ\pi$ to denote the permuted tensor such that $(\Theta\circ\pi)_{i_1,\ldots,i_m} = \Theta_{\pi(i_1),\ldots,\pi(i_m)}$ for all $(i_1,\ldots,i_m)\in[d]^m$. We use $S(d)=\{\pi\colon [d]\to[d]\}$ to denote all possible permutations on $[d]$.
\vspace{-.3cm}
\section{Smooth tensor model with unknown permutation}\label{sec:md}
\vspace{-.3cm}
Suppose we observe an order-$m$ $d$-dimensional symmetric data tensor from the permuted  tensors in \eqref{eq:rep}.
%\begin{equation}\label{eq:rep}
%\tY=\Theta\circ \pi+\tE,\quad\text{where}\quad \Theta_{i_1,\ldots,i_m} = f\left({i_1\over d},\ldots,{i_m\over d}\right).
%\end{equation}
%where $\pi\colon[d]\rightarrow[d]$ is an unknown latent permutation,  $\Theta\in \mathbb{R}^{d\times \cdots\times d}$ is an unknown symmetric signal tensor represented by a multivariate function $f\colon[0,1]^m\rightarrow \mathbb{R}$, and $\tE$ is a symmetric noise tensor consisting of zero-mean, independent sub-Gaussian entries with variance bounded by $\sigma^2$. For simplicity of presentation, we focus on symmetric tensors in the main paper; our models and techniques easily generalize to non-symmetric tensors. 
We assume the generating function $f$ is in the $\alpha$-H\"older smooth family. 
\begin{defn}[$\alpha$-H\"older smooth]
A function $f\colon [0,1]^m\rightarrow \mathbb{R}$ is $\alpha$-H\"older smooth, denoted as $f\in\tH(\alpha)$, if there exists a polynomial $P_{\lfloor \alpha\rfloor}(\mx-\mx_0)$ of degree  $\lfloor \alpha\rfloor$, such that 
\begin{align}\label{eq:defn}
    |f(\mx) - P_{\lfloor \alpha\rfloor}(\mx-\mx_0)| \leq C\|\mx-\mx_0\|_\infty^\alpha, \text{ for all $\mx,\mx_0\in [0,1]^m$ and a constant $C>0.$}
\end{align}

\end{defn}
In addition to the function class $\tH(\alpha)$, we define the smooth tensor class based on discretization~\eqref{eq:rep}, 
\[
{\small \tP(\alpha)= \left\{\Theta\in\mathbb{R}^{d\times \cdots \times d} \colon\Theta(\omega) = f\left({\omega \over d}\right) \text{ for all } \omega=(i_1,\ldots,i_m) \in[d]^m \text{ and } f\in\tH(\alpha)\right\}.}
\]


We give two concrete examples to show the applicability of our permuted smooth tensor model. 

\begin{example}[Four-player game tensor] Consider a four-player board game. Suppose there are in total $d$ players, among which all combinations of four have played against each other. The game results are summarized as an order-4 (asymmetric) tensor, with entries encoding the winner of the games. Our model is then given by
\begin{align}
\mathbb{E}(Y_{i_1,\ldots,i_4})&=\mathbb{P}(\text{user $i_1$ wins over $(i_2,i_3,i_4)$})
=f\left({\pi(i_1)\over d},\cdots, {\pi(i_4)\over d}\right).
\end{align}
In this setting, we can interpret the permutation $\pi$ as the unknown ranking among $d$ players, and the function $f$ the unknown four-players interaction. Operationally, players with similar ranking would have similar performance encoded by the smoothness of $f$. 
\end{example}

\begin{example}[Co-authorship networks] Consider co-authorship networks. Suppose there are in total $d$ authors. We say there exists a hyperedge between nodes $(i_1,\ldots,i_m)$ if the authors $i_1,\ldots,i_m$ have co-authored at least one paper. The resulting hypergraph is represented as an order-$m$ (symmetric) adjacency tensor. Our model is then expressed as
\begin{align}
    \mathbb{E}(Y_{i_1,\ldots,i_m})&=\mathbb{P}(\text{authors $i_1,\ldots,i_m$ co-authored})
=f\left({\pi(i_1)\over d},\cdots, {\pi(i_m)\over d}\right).
\end{align}
In this setting, we can interpret the permutation $\pi$ as the affinity measures of authors, and the function $f$ represents the $m$-way interaction among authors.
%check examples from~\cite{ke2019community}. 
\end{example}



% \subsection{Connection to previous work}\label{sec:priorwork}
% The signal model~\eqref{eq:rep} has close relation to two different problems: multivariate nonparametric regression and hypergraphon estimation. 

% First,  one may view the problem as the classical nonparametric regression problem modeling the mean $(\Theta\circ\pi)_{i_1,\ldots,i_m}$ from  a regression function $f$ with covariates $({i_1}/d,\ldots,{i_m}/d)$. However, in our setting, we cannot observed the permutation $\pi$, which makes the problem  more difficult. Unlike the classical nonparametric regression, the function $f$ should be estimated only from  the observed noisy tensor $\{\tY_{i_1,\ldots,i_m}\}$ without information of the permutation. 
% In addition, this unobserved permutation setting incurs identifiability issue. We cannot estimate both the function $f$ (equivalently $\Theta$) and the permutation $\pi$. To overcome this identifiability issue, we use the following mean squared error for the permuted signal tensor $\Theta\circ\pi$ to measure the performance of the estimation,
% \begin{align}
%     \frac{1}{d^m}\sum_{(i_1,\ldots,i_m)\in[d]^m}\left((\hat\Theta\circ\hat\pi)_{i_1,\ldots,i_m}-(\Theta\circ\pi)_{i_1,\ldots,i_m}\right)^2.
% \end{align}
% Leveraging the underlying structure in \eqref{eq:rep} and smoothness of the function $f$ (formal definition is deferred to Section~\ref{subsec:bm}), we overcome these difficulties and successfully estimate the signal tensor without observing the permutation.

% In addition, the model~\eqref{eq:rep} is related to hypergraphons. 
% A  hypergraph is a generalization of a graph in which an edge (called hyperedge) can join any number of vertices.  $m$-uniform hypergraph is a hypergraph of which all hyperedges have size $m$. The structure of $m$-uniform hypergraph is naturally represented by a $m$-order tensor. So one can view the problem \eqref{eq:gmd} as estimating the probability tensor generating the observed hypergraph.
% The theory of hypergraph limits has studied and introduce hypergraphons  \citep{gowers2007hypergraph,zhao2015hypergraph} similar to graphons, limits of a sequence of graph \citep{lovasz2006limits,diaconis2007graph,lovasz2012large}.    Unlike the matrix case where graphon is represented as a bivariate function~\citep{lovasz2012large}, however, hypergraphons for $m$-uniform hypergraphs should be represented as a $(2^m-2)$-variate function \citep{zhao2015hypergraph}. Due to exponential number of variates, the general hypergraphon suffers from  high sample complexity. 
% To overcome this issue, simple $m$-variate hypergraphons have been introduced recently for efficient estimation while trading off the model flexibility \citep{balasubramanian2021nonparametric,lyu2021latent}.
%  Although our model can be viewed as a simple $m$-variate hypergraphon, our main interests lie on the general estimation of the smooth signal tensor from noisy observation including both binary- and continuous-valued entries.
\vspace{-.4cm}
\section{Block-wise tensor approximation }\label{sec:tba}
\vspace{-.3cm}
Our general strategy for estimating the signal tensor is based on the block-wise tensor approximation. We first introduce the tensor block model~\citep{wang2019multiway,han2020exact}. Then, we extend this model to the block-wise polynomial approximation.


{\bf Tensor block model.} The tensor block model describes a checkerbroad pattern in the signal tensor. Specifically, suppose that there are $k$ clusters in the tensor dimension $d$, and the clusters are represented by a clustering function $z\colon [d]\rightarrow  [k]$. Then, the tensor block model assumes that signal tensor $\Theta\in\mathbb{R}^{d\times \cdots \times d}$ takes values from a mean tensor $\tS\in\mathbb{R}^{k\times\cdots\times k}$ according to the clustering function $z$:
\begin{align}\label{eq:block}
    \Theta_{i_1,\ldots,i_m} = \tS_{z(i_1),\ldots,z(i_m)}, \quad \text{ for all } (i_1,\ldots,i_m)\in[d]^m.
\end{align}
A tensor $\Theta$ satisfying~\eqref{eq:block} is called a block-$k$ tensor. Tensor block model have shown great success in discoverying hidden group structure for many applications~\citep{wang2019multiway,han2020exact}.
Despite its popularity and great applicability, the tensor block models cannot describe delicate structure of the signal tensor when the tensor dimension $d$ is very large. 
This parametric model aims to explain data with a finite number of blocks; this approach is useful when the sample outsizes the parameters. Our nonparametric models~\eqref{eq:rep}, by contrast, use infinite number of parameters to allow growing model complexity as sample increases. 
Therefore, we shift the goal of tensor block model from discovering hidden group structure to approximating the generative process of the function $f$ in~\eqref{eq:rep}. Thus, the number of blocks $k$ should be interpreted as a resolution parameter (i.e., a bandwidth) of the approximation similar to the notion of number of bins in histogram and polynomial regression. 



{\bf Block-wise polynomial approximation.} The tensor block model~\eqref{eq:block} can be viewed as a discrete version of piece-wise \emph{constant} function. This connection motivates us to use block-wise \emph{polynomial} tensors to approximate $\alpha$-H\"older functions.  For a given block number $k$, we use $z\colon[d]\rightarrow [k]$ to denote the canonical clustering function that partitions $[d]$ into $k$ clusters,  $z(i) = \lceil ki/d\rceil,  \text{ for all } i\in[d].$
The collection of inverse images $\{z^{-1}(j)\colon j\in[k]\}$ consists of  disjoint and equal-sized subsets in $[d]$, and we have $\cup_{j\in[k]}z^{-1}(j) = [d]$ by the construction. We denote $\tE_k$ as the $m$-way partition as a collection of $k^m$ disjoint, equal-sized blocks in $[d]^m$, such that 
\begin{align}\label{eq:blockind}
    \tE_k = \{z^{-1}(j_1)\times\cdots\times z^{-1}(j_m)\colon (j_1,\ldots,j_m)\in [k]^m\}.
\end{align}
We propose to approximate the signal tensor $\Theta$ in~\eqref{eq:rep} by degree-$\ell$ polynomial tensor within each $\tE_k$-block. Specifically, we use $\caliB(k,\ell)$ to denote the class of block-$k$, degree-$\ell$ polynomial tensors,
\begin{align}
    \caliB(k,\ell) = \bigg\{&\tB\in(\mathbb{R}^d)^{\otimes m}\colon \tB(\omega) = \sum_{\Delta\in\tE_k}\text{Poly}_{\ell,\Delta}(\omega)\mathds{1}\{\omega\in\Delta\}\text{ for all } \omega\in[d]^m\bigg\},
\end{align}
where $\text{Poly}_{\ell,\Delta}(\cdot)$ denotes a degree-$\ell$ polynomial function in $\mathbb{R}^m$. Notice that degree-0 polynomial block tensor reduces to the tensor block model \eqref{eq:block}. We genealized the tensor block model to degree-$\ell$ polynomial block tensor, in a way analogous to the generalization from $k$-bin histogram to $k$-piece-wise polynomial regression.

%Smoothness of the function $f$ in \eqref{eq:rep} turns out to play an important role in the block-wise polynomial approximation.  The following lemma implies that we can always find block-wise polynomial tensor close to the signal tensor generated from $\alpha$-H\"older smooth function $f$.
%\begin{lem}[Tensor block approximation]\label{lem:approx}
%Suppose  $\Theta\in\tP(\alpha)$. Then,
%for every block number $k\leq d$, and degree $\ell\in\{0\}\cup\mathcal{N}_+$, we have the approximation error
%\begin{align}
%   \inf_{\tB\in\caliB(k,\ell)} \frac{1}{d^m}\FnormSize{}{\Theta-\tB}^2\lesssim \frac{m^2}{k^{2\min(\alpha,\ell+1)}}.
%\end{align}
%\end{lem}



\vspace{-.3cm}
\section{Fundamental limits via least-squares estimation}\label{sec:lse}
\vspace{-.3cm}
We propose two estimation methods based on the block-wise polynomial approximation. We first introduce a minimax optimal but computationally infeasbile least-squares estimator as statistical benchmark. In Section~\ref{sec:borda}, we will present a polynomial-time algorithm with provably same optimal rates under monotonicity assumptions.

We propose the least-squares estimator for the signal tensor and the permutation $(\Theta,\pi)$ by minimizing the Frobenius loss under block-$k$, degree-$\ell$ polynomial tensor family $\caliB(k,\ell)$, 
\begin{align}\label{eq:lseopt}
    (\hat\Theta^{\text{LSE}},\hat \pi^{\text{LSE}}) &= \argmin_{\Theta\in\caliB(k,\ell), \  \pi\in S(d)}\FnormSize{}{\tY-\Theta\circ\pi}.
\end{align}
The least-squares estimator $(\hat\Theta^{\text{LSE}},\hat\pi^{\text{LSE}})$ depends on two tuning parameters: the number of blocks $k$ and the polynomial degree $\ell$. The optimal choice $(k^*,\ell^*)$ is provided in our next theorem. The result establishes the upper bound for the mean squared error of the least square estimator~\eqref{eq:lseopt}. 
\begin{thm}[Least-squares estimation error]\label{thm:LSE} 
Consider the order-$m$ ($m\geq 2$) permuted smooth tensor model~\eqref{eq:rep} with $\Theta\in\tP(\alpha)$.
Then, the estimator $\hat\Theta^{\textup{LSE}}\circ\hat\pi^{\textup{LSE}}$  in \eqref{eq:lseopt} satisfies with high probability 
\begin{align}\label{eq:rates}
   \frac{1}{d^m}&\FnormSize{}{\hat\Theta^{\textup{LSE}}\circ\hat\pi^{\textup{LSE}}-\Theta\circ \pi}^2\lesssim  \begin{cases}d^{-{2m\alpha\over m+2\alpha}} & \text{ when } \alpha < \frac{m(m-1)}{2},\\{\log d\over d^{m-1}}&\text{ when } \alpha \geq \frac{m(m-1)}{2}.\end{cases},
\end{align}
under the optimal choice of $\ell^* = \min(\lfloor\alpha\rfloor,(m-2)(m+1)/2)$ and $k^* = \lceil d^{m\over m+2\min(\alpha,\ell^*+1)}\rceil$.
\end{thm}
We discuss the asymptotic error rates as $d\rightarrow \infty$ while treating the tensor order $m$ and smoothness $\alpha$ fixed. 
The least square estimation error has two sources of error: the nonparametric error $d^{-{2m\alpha\over m+2\alpha}}$ and the clustering error $\log d/d^{m-1}$.  When the function $f$ is smooth enough, estimating the function $f$ becomes relatively easier compared to estimating the permutation $\pi$. This intuition coincides with the fact that the clustering error dominates the nonparametric error when  $\alpha\geq m(m-1)/2$. 


We now compare our results with existing work in the literature. Based on Theorem~\ref{thm:LSE}, the best rate is obtained with the choice of  $(\ell^*, k^*) = (0,\lceil d^{1\over\alpha\wedge1+1}\rceil)$ in the matrix case ($m =2$). This block-wise constant approximation and convergence rate reduce to the results in  \cite{gao2016optimal,klopp2017oracle}. We improve the conjectured optimal convergence rate in \cite{balasubramanian2021nonparametric} for higher order tensor case ($m\geq 3$). This improvement stems from polynomial tensor approximation. The work in
\cite{balasubramanian2021nonparametric} considers only the block-wise constant approximation.  This restriction results in sub-optimality because the optimal degree $\ell^*$ is shown to be greater than 0 for higher-order tensors.  This result shows the clear difference from matrices and highlights the challenges with  tensors. 

We show that the upper bound of Theorem~\ref{thm:LSE} is  minimax optimal.
The result is based on information-theoretical analysis that combines the minimax rate for nonparametric and permutation estimation.
\begin{thm}[Minimax lower bound]\label{thm:minimax}For any given $\alpha\in(0,\infty)$, the estimation problem based on model~\eqref{eq:rep} obeys the minimax lower bound 
\begin{equation}\label{eq:minimax}
\inf_{(\hat \Theta,\hat \pi)}\sup_{\Theta\in \tP(\alpha), \pi\in S(d)} \mathbb{P}\left({1\over d^m}\FnormSize{}{\Theta\circ \pi-\hat \Theta\circ \hat \pi}^2 \gtrsim d^{-{2m\alpha\over m+2\alpha}}+d^{-(m-1)}\log d \right) \geq 0.8.
\end{equation}
\end{thm}



% There are two ingredients in the rate of convergence, the nonparametric rate $ d^{-2m(\alpha\wedge 1)\over m+2(\alpha\wedge 1)}$ and the clustering rate $\log d/d^{m-1}$.
% Depending on constants tensor order $m$ and smoothness $\alpha$, the convergence rate in \eqref{eq:rateMSE} becomes 
% \begin{align}
%      d^{-2m(\alpha\wedge1)\over m+2(\alpha\wedge1)}+\frac{\log d}{d^{m-1}}\asymp \begin{cases} d^{-2\alpha\over 1+\alpha}& m = 2, \alpha\in(0,1),\\ \log d/d & m =2, \alpha = 1,\\ d^{-2m(\alpha\wedge 1)\over m+2(\alpha\wedge 1)} & m>2.\end{cases}
% \end{align}
% Notice that the nonparametric rate dominates the clustering rate when $m>2$. The intuitive  explanation for this phenomenon is that when tensor order $m$ increases, the nonparametric estimation becomes harder because the number of parameters increases exponentially due to the possible $m$-combinations from $k$ clusters. On the other hand, the difficulty of clustering problem are almost the same regardless of tensor order $m$. This is because the number of clusters $k$ and tensor dimension $d$ remain the same.


\vspace{-.5cm}
\section{An adaptive and computationally feasible procedure}\label{sec:borda}
\vspace{-.3cm}
At this point, we should point out that computing the least square optimizer in \eqref{eq:lseopt} with polynomial-time algorithm is unknown. We suspect that the algorithm for \eqref{eq:lseopt} may be computationally intractable. In this section, we propose an efficient polynomial-time \emph{Borda count} algorithm with provably same optimal rate under the $\beta$-monotonicity condition.  We first introduce $\beta$-monotonicity condition.  
\begin{defn}[$\beta$-monotonicity]\label{eq:defn}
A function $f\colon[0,1]^m \rightarrow \mathbb{R}$ is called $\beta$-monotonic, denoted as $f\in\tM(\beta)$, if 
\begin{align}\label{eq:monotonic}
    \left({i-j\over d}\right)^{1/\beta}\leq g(i)-g(j),\text{for all $i>j\in[d]$ where } g(i): = \frac{1}{d^{m-1}}\sum_{(i_2,\ldots,i_m)\in[d]^m} f\left({i\over d},{i_2\over d},\ldots,{i_m\over d}\right).
\end{align}
\end{defn}\vspace{-.3cm}
This $\beta$-monotonicity condition can be viewed as an extension of the strict monotonic degree condition in binary-valued networks~\citep{chan2014consistent} to general setting.  Our $\beta$-monotonicity condition is also closely related to isotonic functions~\citep{han2019isotonic,pananjady2020isotonic} which assume the coordinate-wise monotonicity, i.e., $f(x_1,\ldots,x_d)\leq f(x_1',\ldots,x_d')$
when $x_i\leq x_i'$ for $i\in[d]$.  This $\beta$-monotonicity condition allows to estimate the permutation $\pi$  in polynomial-time. 

%Before presenting the theoretical guarantees, we provide the intuition here. The parameter $\beta$ measures the difficulty of the problem for estimating the permutation $\pi$. Consider the noisy observation $\tY$ in  \eqref{eq:gmd}.
%We define the scores function $\tau\colon [d]\rightarrow\mathbb{R}$ as
%\begin{align}\label{eq:score}
%    \tau(i) = \frac{1}{d^{m-1}}\sum_{(i_2,\ldots,i_m)\in[d]^m} \tY_{i,i_2,\ldots,i_m}.
%\end{align}
%Then, the permuted score function $\tau\circ\pi^{-1}$ is equivalent to the function $g$ in \eqref{eq:monotonic} for the noiseless case.
%Therefore, we can find an estimate $\hat\pi$ that makes the permuted score function $\tau\circ\hat\pi^{-1}$ monotonically increasing. Notice that the estimated permutation $\hat\pi$ could be different from the oracle permutation $\pi$ due to the noise. We find that the larger $\beta$ guarantees the sharper consistency of $\hat\pi$. The large $\beta$ implies the large gaps of $|g(i)-g(j)|$ for $i\neq j\in[d]$. Therefore, we obtain similar ordering of $\{\tau(i)\}_{i=1}^d$ before and after the addition of the noise. This intuition is well represented by the following lemma.
%\begin{lem}[Permutation error]\label{lem:permute}
%Let $\hat\pi$ be the permutation that makes the permuted score function $\tau\circ \hat\pi^{-1}$  monotonically increasing. Then, we have
%\begin{align}
%   \textup{Loss}(\pi,\hat\pi):= \frac{1}{d}\max_{i\in[d]}|\pi(i)-\hat\pi(i)|\lesssim \left(\sigma d^{-(m-1)/2}\sqrt{\log d}\right)^{\beta},
%\end{align}
%with high probability.
%\end{lem}


Now we introduce a Borda count estimator that consists of two stages: sorting and block-wise polynomial approximation. The simplified version of the algorithm is described in Algorithm 1. 
\vspace{-.3cm}
\begin{center}
\includegraphics[width=.92\textwidth]{figures/algorithm_template.pdf}
\end{center}
\vspace{-.4cm}
 Notice that the least square estimation in \eqref{eq:lseopt} requires combinatoric search for the permutation resulting in exponential time complexity. However, optimization problem in Algorithm 1 only requires to estimate the degree-$\ell$ polynomial block tensor. Therefore, this step reduces to a degree-$\ell$ polynomial regression problem within each block $\tE_k$.  The full estimation procedure is illustrated in Figure~\ref{fig:borda}.
\begin{figure}[htp]
    \centering
    \includegraphics[width = .8\textwidth]{figures/Borda2.pdf}
    \caption{Procedure of Borda count estimation. We first sort the tensor entries using the proposed procedure. Then, we estimate the signal tensor using block-$k$ degree-$\ell$ polynomial approximation.}
    \label{fig:borda}
    \vspace{-.6 cm}
\end{figure}
%\vspace{-.2cm}
%\subsection{Computational and statistical complexity}
%\vspace{-.2cm}
%%\paragraph{Complexity of the algorithm.}
%The complexity of the Borda count algorithm can be computed separately in each stage.
%In the sorting stage, computing the score function $\tau$  requires $\tO(d^{m-1})$ additions while sorting the $\tau(1),\ldots,\tau(d)$ takes about $\tO(d\log d)$ comparisons. In block-wise polynomial approximation stage, we compute $k^m$ different  degree-$\ell$ polynomial tensors. For each  degree-$\ell$ polynomial tensor,  $\tO((d/k)^m\ell)$ arithmetic operations are needed. Thus, the second step requires $\tO(d^m\ell)$ arithmetic operations.  Combining these two steps yields the total complexity at most $\tO(d^m\log d)$.

%\paragraph{Consistency of Borda count estimation.}
We show the consistency of the signal tensor estimation of Borda count estimator.
\begin{thm}[Estimation error for Borda count]\label{thm:BC}
Suppose that the signal tensor $\Theta$ is generated as in  \eqref{eq:rep} with $f\in\tH(\alpha)\cap \tM(\beta).$
Then estimators $(\hat\Theta^{\text{BC}},\hat\pi^{\text{BC}})$ from Algorithm 1 satisfies
\begin{align}\label{eq:BC}
   \frac{1}{d^m}&\FnormSize{}{\hat\Theta^{\textup{BC}}\circ\hat\pi^{\textup{BC}}-\Theta\circ \pi}^2\lesssim  \begin{cases}d^{-{2m\alpha\over m+2\alpha}} & \text{ when } \alpha <c(\alpha,\beta,m),\\{\log d\over d^{m-1}}&\text{ when } \alpha \geq c(\alpha,\beta,m).\end{cases},
\end{align}
with high probability under the optimal choice of $\ell^* = \min(\lfloor\alpha\rfloor,\lfloor c(\alpha,\beta,m)\rfloor)$ and $k^* = \lceil d^{m\over m+2\min(\alpha,\ell^*+1)}\rceil$.  Here $c(\alpha,\beta,m)>0$ is a constant only depending on $\alpha,\beta$,  and $m$.
\end{thm}
% with probability  $1-\exp\left(-C_2\left(d\log d +d^{m^2\over m+2(\alpha\wedge 1)}\right)\right)$.
We find that the Borda count estimator achieves the same minimax-optimal rate as the least-squares estimator for sufficiently smooth tensors under Lipschitz score condition $\beta =1$. The least-squares estimator requires a combinatoric search with exponential-time complexity. By contrast, the Borda count estimator is polynomial-time solvable. Therefore, Borda count algorithm enjoys both statistical accuracy and computational efficiency. 


\vspace{-.3cm}
\section{Numerical experiments and data application}\label{sec:sim}
\vspace{-.3cm}
{\bf Numerical experiments.} We simulate symmetric order-3 $d$-dimensional tensors based on the permuted smooth tensor model~\eqref{eq:rep} with function $f$ in Figure~\ref{fig:area}a. Notice that considered functions cover a reasonable range of model complexities from low rank to high rank. We generate the entries of the noise tensor i.i.d. from Gaussian distribution $N(0,0.5^2)$. The permutation $\pi$ is randomly sampled from all permutations. 
Throughout all experiments, we evaluate the accuracy of the estimation by mean square error (MSE) $= d^{-3}\FnormSize{}{\Theta\circ\pi-\hat\Theta\circ\hat\pi}^2$.

\vspace{-.3cm}
\begin{figure}[htp]
    \centering
    \includegraphics[width =\textwidth]{figures/sim.pdf}    
    \caption{Panels a-c: MSE comparison versus the number of blocks for different polynomial approximation under models 1-3 respectively.  Panel d-f:  MSE comparison of different methods versus tensor dimension under models 1-3 respectively. MSEs are measured across $n_{\text{sim}} = 20$ replications.}
    \label{fig:degk}
    \vspace{-.3cm}
\end{figure}

The first experiment examines the impact of the block number $k$ and degree of polynomial $\ell$ for the approximation. We fix the tensor dimension $d = 100$, and vary the number of blocks $k\in\{1,\ldots,15\}$ and polynomial degree $\ell\in\{0,1,2,3\}.$
Figure~\ref{fig:degk}a-c demonstrates the trade-off in accuracy determined by the number of groups for each polynomial degree. In addition, we find that degree-2 polynomial approximation with the optimal $k$ gives the smallest MSE among all considered polynomial approximation. These two observations are well explained by the optimal choice of $(k^*,\ell^*) = (\tO(\lceil d^{3/7}\rceil,2)$ in our theoretical results. 
The second experiment compares our method ({\bf \small Borda Count}) with several popular alternative methods: (a) Spectral method ({\bf \small Spectral})~\citep{xu2018rates} on unfolded tensor; (b) Least square estimation ({\bf \small LSE})~\cite{balasubramanian2021nonparametric} with $\ell=0$; (c) Our {\bf \small Borda Count} algorithm. 
Figure~\ref{fig:degk}d-f shows that our algorithm {\bf \small Borda Count} achieves the best performance in all scenarios as the tensor dimension increases. The poor performance of {\bf \small Spectral} can be explained by the loss of multilinear structure in the tensor unfolding procedure. The sub-optimality of {\bf \small LSE} is possibly due to its limits in both statistics and computations. Statistically, our theorems have shown that constant block approximation has sub-optimal rates. Computationally, the least square optimization~\eqref{eq:lseopt} is highly non-convex and computationally unstable. The outperformance of {\bf \small Borda count} demonstrates the efficacy of our method.

\begin{figure}[h]
    \centering
    \includegraphics[width =.87 \textwidth]{figures/drawing.pdf}
    \caption{Panel a: smooth functions in simulations. Panels b-c: Chicago crime maps. Panel b shows homicides and shooting incidents in community areas in Chicago. This figure is from \textit{Chicago Tribune} article in 2020 \citep{Jeremy.2020}. Panel c shows the four areas estimated by our Borda Count algorithm. }
    \label{fig:area}
    \vspace{-.6cm}
\end{figure}

{\bf Applications to Chicago crime data.} Chicago crime tensor dataset is an order-3 tensor with entries representing the log counts of crimes from 24 hours, 77 Chicago community areas, and 32 crime types ranging from January 1st, 2001 to December 11th, 2017. We apply our Borda Count method to Chicago crime dataset.  Because the data tensor is asymmetric, we allow different number of blocks across the three modes. Cross validation result suggests the $(k_1,k_2,k_3)=(6,4,10)$, representing the block number for crime hours, community areas, and crime types, respectively.
We investigate the four clustered community areas obtained from our Borda Count algorithm.  Figure~\ref{fig:area}c shows the four areas overlaid on a map of Chicago. Interestingly,  we find that the clusters conform the actual locations even though our algorithm did not take any geographic information such as longitude or latitude. In addition, we compare the cluster patterns with benchmark results based on homicides- and shooting incidents-maps in Chicago shown in Figure~\ref{fig:area}b. We find that our clusters share similar geographical patterns with Figure~\ref{fig:area}b. The benchmark Figure~\ref{fig:area}b covers only homicides and shooting incidents in 2020, whereas our result in Figure~\ref{fig:area}c considers 32 crime types across 2001-2017. The results demonstrate the power of our approach in detecting meaningful pattern from tensor data. 

% \fixme{Miaoyan}{change ``Source; authors' calculation'' ``Sources: our calculation''.}


%Then, we examine the denoised signal tensor obtained from our method and analyze the trends between crime types and crime hours by the four community areas in Figure~\ref{fig:area}(b). Figure~\ref{fig:crimeA} shows the averaged log counts of crimes according to crime types and crime hours by four areas. We find that the major difference among four areas is the crime rates. Area 4 has the highest crime rates,  and the crime rates monotonically decrease from Area 4 to Area 1. The variation in crime rates across hour and type, nevertheless, exhibits similarity among the four areas. For example, Figure~\ref{fig:crimeA} shows that the number of crimes increases hourly from 8 p.m., peaks at night hours, and then drops to the lowest at 6 p.m. 
%The identified similarities and differences among the four community areas highlight the interpretability of our method in real data.
%\begin{figure}[h]
%    \centering
%    \includegraphics[width = \textwidth]{figures/CrimeAn.pdf}
%    \caption{Averaged log counts of crimes according to crime types, hours, and the four areas estimated by our Borda Count algorithm. We plot the estimated signal tensor entries averaged within four areas in the heatmap.}
%    \label{fig:crimeA}
%    \vspace{-.4cm}
%\end{figure}


\vspace{-.4cm}
\section{Conclusion}\label{sec:con}
\vspace{-.4cm}
We have developed permuted smooth tensor model and estimation methods with theoretical guarantees. 
%An optimal error bound for the least square estimation is established based on block-wise polynomial approximation. Borda count estimation with polynomial algorithm is provided with the same convergence rate under $\beta$-monotonicity. 
The efficacy of our procedure is demonstrated through both simulations and analysis of Chicago crime dateset. 



% Acknowledgements should go at the end, before appendices and references

%\acks{We would like to acknowledge support for this project from the National Science Foundation (NSF grant IIS-9988642) and the Multidisciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.



% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            

\vspace{-.5cm}

\bibliographystyle{plain}
\bibliography{tensor_wang}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Checklist}

% %%% BEGIN INSTRUCTIONS %%%
% The checklist follows the references.  Please
% read the checklist guidelines carefully for information on how to answer these
% questions.  For each question, change the default \answerTODO{} to \answerYes{},
% \answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
% justification to your answer}, either by referencing the appropriate section of
% your paper or providing a brief inline description.  For example:
% \begin{itemize}
%   \item Did you include the license to the code and datasets? \answerYes{}
%   \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
%   \item Did you include the license to the code and datasets? \answerNA{}
% \end{itemize}
% Please do not modify the questions and only use the provided macros for your
% answers.  Note that the Checklist section does not count towards the page
% limit.  In your paper, please delete this instructions block and only keep the
% Checklist section heading above along with the questions/answers below.
% %%% END INSTRUCTIONS %%%

% \begin{enumerate}

% \item For all authors...
% \begin{enumerate}
%   \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \answerYes{} See Section~\ref{sec:int}, {\bf Contributions.}
%   \item Did you describe the limitations of your work?
%     \answerYes{} We provide the limitation of the least square estimation in  Section~\ref{sec:borda}
%   \item Did you discuss any potential negative societal impacts of your work?
%     \answerNA{} This work does not present any foreseeble societal consequence
%   \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
%     \answerYes{}
% \end{enumerate}

% \item If you are including theoretical results...
% \begin{enumerate}
%   \item Did you state the full set of assumptions of all theoretical results?
%     \answerYes{} The assumptions for main Theorems are fully state.
% 	\item Did you include complete proofs of all theoretical results?
%     \answerNo{}
% \end{enumerate}

% \item If you ran experiments...
% \begin{enumerate}
%   \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
%     \answerNo{}
%   \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
%     \answerYes{} Training procedures are described in Section~\ref{sec:sim}
% 	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
%     \answerYes{} Error bars are provided in all Figures
% 	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
%     \answerNo{}
% \end{enumerate}

% \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
% \begin{enumerate}
%   \item If your work uses existing assets, did you cite the creators?
%     \answerYes{} Data source has been cited in Section~\ref{sec:app}
%   \item Did you mention the license of the assets?
%     \answerNA{} No license available for teh assets used in this work.
%   \item Did you include any new assets either in the supplemental material or as a URL?
%     \answerTODO{}
%   \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
%     \answerNA{} The datasets are publicly available.
%   \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
%     \answerNA{} No personally identifiable information is included in the datsets.
% \end{enumerate}

% \item If you used crowdsourcing or conducted research with human subjects...
% \begin{enumerate}
%   \item Did you include the full text of instructions given to participants and screenshots, if applicable?
%     \answerNA{} This work does not involve human subjects.
%   \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
%     \answerNA{} This work does not involve potential participant risk
%   \item Did you include the estimated hourly wage paid to participants. and the total amount spent on participant compensation?
%     \answerNA{} This work does not involve participants.
% \end{enumerate}

% \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}