\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baltrunas2011incarmusic,bi2018multilayer}
\citation{bickel2009nonparametric}
\citation{hore2016tensor}
\citation{zhou2013tensor}
\citation{baltrunas2011incarmusic}
\citation{young2018universality}
\citation{wang2019multiway,han2020exact}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\newlabel{eq:gmd}{{1}{2}{Introduction}{equation.1.1}{}}
\citation{gao2015rate,klopp2017oracle}
\citation{balasubramanian2021nonparametric,li2019nearest}
\citation{balasubramanian2021nonparametric,gao2015rate}
\citation{balasubramanian2021nonparametric}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a): Illustration of order-$m$ $d$-dimensional permuted smooth tensor models with $m=2$. (b): Phase transition of mean squared error (MSE) (on $-\qopname  \relax o{log}_d$ scale) as a function of smoothness $\alpha $ and tensor order $m$. Bold dots correspond to the critical smoothness level above which higher smoothness exhibits no further benefits to tensor estimation. \relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rate}{{1}{3}{(a): Illustration of order-$m$ $d$-dimensional permuted smooth tensor models with $m=2$. (b): Phase transition of mean squared error (MSE) (on $-\log _d$ scale) as a function of smoothness $\alpha $ and tensor order $m$. Bold dots correspond to the critical smoothness level above which higher smoothness exhibits no further benefits to tensor estimation. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Our contributions}{3}{subsection.1.1}\protected@file@percent }
\MT@newlabel{eq:gmd}
\citation{pananjady2020isotonic}
\citation{balasubramanian2021nonparametric}
\citation{li2019nearest}
\citation{klopp2017oracle,gao2015rate}
\citation{tsybakov2009introduction}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of our results with previous work. $^*$For simplicity, we list here the error rate (omitting the log term) for $\infty $-smooth tensors. Our results allow general tensors of arbitrary smoothness level $\alpha \geq 0$; See Theorems\nobreakspace  {}\ref  {thm:LSE}-\ref  {thm:BC} in Sections\nobreakspace  {}\ref  {sec:lse}-\ref  {sec:borda}.\relax }}{4}{table.caption.2}\protected@file@percent }
\newlabel{tab:comp}{{1}{4}{Comparison of our results with previous work. $^*$For simplicity, we list here the error rate (omitting the log term) for $\infty $-smooth tensors. Our results allow general tensors of arbitrary smoothness level $\alpha \geq 0$; See Theorems~\ref {thm:LSE}-\ref {thm:BC} in Sections~\ref {sec:lse}-\ref {sec:borda}.\relax }{table.caption.2}{}}
\MT@newlabel{eq:gmd}
\MT@newlabel{eq:gmd}
\citation{chan2014consistent,klopp2017oracle}
\citation{chatterjee2015matrix,shah2019low}
\citation{flammarion2019optimal,hutter2020estimation}
\citation{ding2021efficient,livi2013graph}
\citation{pananjady2020isotonic}
\citation{balasubramanian2021nonparametric,li2019nearest}
\citation{balasubramanian2021nonparametric}
\citation{li2019nearest}
\citation{kolda2009tensor,sun2017provable}
\citation{zhang2018tensor}
\citation{wang2019multiway}
\citation{pananjady2020isotonic,li2019nearest,gao2015rate,bickel2009nonparametric,shah2019low}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Related work}{5}{subsection.1.2}\protected@file@percent }
\newlabel{sec:priorwork}{{1.2}{5}{Related work}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Structure learning with latent permutations.}{5}{section*.3}\protected@file@percent }
\MT@newlabel{eq:gmd}
\@writefile{toc}{\contentsline {paragraph}{Low-rank tensor models.}{5}{section*.4}\protected@file@percent }
\citation{tsybakov2009introduction}
\citation{klopp2017oracle,gao2015rate,chan2014consistent}
\citation{zhao2015hypergraph,lovasz2012large}
\citation{lovasz2012large}
\citation{zhao2015hypergraph}
\citation{balasubramanian2021nonparametric}
\citation{wasserman2006all,tsybakov2009introduction}
\@writefile{toc}{\contentsline {paragraph}{Nonparametric regression.}{6}{section*.5}\protected@file@percent }
\MT@newlabel{eq:gmd}
\@writefile{toc}{\contentsline {paragraph}{Graphon and hypergraphon.}{6}{section*.6}\protected@file@percent }
\MT@newlabel{eq:gmd}
\MT@newlabel{eq:gmd}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Notation and organization}{7}{subsection.1.3}\protected@file@percent }
\citation{wasserman2006all,tsybakov2009introduction}
\@writefile{toc}{\contentsline {section}{\numberline {2}Smooth tensor model with unknown permutation}{8}{section.2}\protected@file@percent }
\newlabel{sec:model}{{2}{8}{Smooth tensor model with unknown permutation}{section.2}{}}
\newlabel{eq:obs}{{2}{8}{Smooth tensor model with unknown permutation}{equation.2.2}{}}
\MT@newlabel{eq:obs}
\newlabel{eq:rep}{{3}{8}{Smooth tensor model with unknown permutation}{equation.2.3}{}}
\newlabel{eq:defn}{{4}{8}{$\alpha $-H\"older smooth}{equation.2.4}{}}
\citation{klopp2017oracle,gao2015rate}
\citation{chen2021optimal}
\MT@newlabel{eq:rep}
\MT@newlabel{eq:obs}
\MT@newlabel{eq:rep}
\citation{wang2018learning}
\citation{wang2019multiway,han2020exact}
\citation{young2018universality}
\citation{wang2018learning}
\citation{pananjady2020isotonic}
\@writefile{toc}{\contentsline {section}{\numberline {3}Block-wise tensor estimation}{10}{section.3}\protected@file@percent }
\newlabel{sec:tba}{{3}{10}{Block-wise tensor estimation}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Tensor block model}{10}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:bm}{{3.1}{10}{Tensor block model}{subsection.3.1}{}}
\newlabel{eq:block}{{5}{10}{Tensor block model}{equation.3.5}{}}
\citation{wang2019multiway,han2020exact}
\citation{bickel2009nonparametric,gao2015rate}
\citation{wang2019multiway,han2020exact}
\citation{wasserman2006all}
\MT@newlabel{eq:block}
\MT@newlabel{eq:block}
\MT@newlabel{eq:block}
\MT@newlabel{eq:rep}
\MT@newlabel{eq:rep}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Block-wise polynomial approximation}{11}{subsection.3.2}\protected@file@percent }
\newlabel{sec:poly}{{3.2}{11}{Block-wise polynomial approximation}{subsection.3.2}{}}
\MT@newlabel{eq:block}
\MT@newlabel{eq:defn}
\citation{wasserman2006all}
\MT@newlabel{eq:block}
\newlabel{eq:blockind}{{3.2}{12}{Block-wise polynomial approximation}{subsection.3.2}{}}
\MT@newlabel{eq:rep}
\newlabel{eq:polynomial}{{6}{12}{Block-wise polynomial approximation}{equation.3.6}{}}
\MT@newlabel{eq:block}
\MT@newlabel{eq:polynomial}
\MT@newlabel{eq:rep}
\newlabel{lem:approx}{{1}{12}{Block-wise polynomial tensor approximation}{lem.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Fundamental limits via least-squares estimation}{13}{section.4}\protected@file@percent }
\newlabel{sec:lse}{{4}{13}{Fundamental limits via least-squares estimation}{section.4}{}}
\MT@newlabel{eq:obs}
\newlabel{eq:lseopt}{{7}{13}{Fundamental limits via least-squares estimation}{equation.4.7}{}}
\MT@newlabel{eq:lseopt}
\newlabel{thm:LSE}{{1}{13}{Least-squares estimation error}{thm.4.1}{}}
\MT@newlabel{eq:obs}
\MT@newlabel{eq:lseopt}
\newlabel{eq:rateMSE}{{8}{13}{Least-squares estimation error}{equation.4.8}{}}
\citation{tsybakov2009introduction}
\MT@newlabel{eq:rateMSE}
\newlabel{eq:rates}{{9}{14}{Least-squares estimation error}{equation.4.9}{}}
\MT@newlabel{eq:rates}
\newlabel{eq:m1}{{10}{14}{Comparison to non-parametric regression}{equation.4.10}{}}
\citation{bickel2009nonparametric,gao2015rate,klopp2017oracle}
\citation{balasubramanian2021nonparametric}
\citation{balasubramanian2021nonparametric}
\MT@newlabel{eq:m1}
\MT@newlabel{eq:rates}
\MT@newlabel{eq:lseopt}
\newlabel{thm:minimax}{{2}{15}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:gmd}
\newlabel{eq:minimax}{{11}{15}{Minimax lower bound}{equation.4.11}{}}
\MT@newlabel{eq:minimax}
\MT@newlabel{eq:rates}
\MT@newlabel{eq:lseopt}
\citation{gao2015rate}
\citation{chan2014consistent}
\citation{pananjady2020isotonic}
\newlabel{rmk:phase}{{3}{16}{Phase transition}{rmk.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}An efficient and computationally feasible procedure}{16}{section.5}\protected@file@percent }
\newlabel{sec:borda}{{5}{16}{An efficient and computationally feasible procedure}{section.5}{}}
\MT@newlabel{eq:lseopt}
\MT@newlabel{eq:minimax}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Borda count algorithm}{16}{subsection.5.1}\protected@file@percent }
\newlabel{eq:bdefn}{{2}{16}{Weakly $\beta $-monotonicity}{defn.2}{}}
\newlabel{eq:monotonic}{{12}{16}{Weakly $\beta $-monotonicity}{equation.5.12}{}}
\MT@newlabel{eq:monotonic}
\MT@newlabel{eq:gmd}
\newlabel{eq:score}{{5.1}{17}{Borda count algorithm}{equation.5.12}{}}
\MT@newlabel{eq:monotonic}
\newlabel{lem:permute}{{2}{17}{Permutation error}{lem.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of Borda count estimation. We first sort tensor entries using the proposed procedure, and then estimate the signal by block-wise polynomial approximation.\relax }}{18}{figure.caption.7}\protected@file@percent }
\newlabel{fig:borda}{{2}{18}{Illustration of Borda count estimation. We first sort tensor entries using the proposed procedure, and then estimate the signal by block-wise polynomial approximation.\relax }{figure.caption.7}{}}
\newlabel{eq:permute}{{13}{18}{Borda count algorithm}{equation.5.13}{}}
\newlabel{eq:bclse}{{14}{18}{Borda count algorithm}{equation.5.14}{}}
\MT@newlabel{eq:polynomial}
\MT@newlabel{eq:lseopt}
\citation{li2019nearest,zhang2018tensor}
\MT@newlabel{eq:bclse}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Computational and statistical complexities}{19}{subsection.5.2}\protected@file@percent }
\newlabel{thm:BC}{{3}{19}{Estimation error for Borda count algorithm}{thm.5.3}{}}
\MT@newlabel{eq:permute}
\MT@newlabel{eq:bclse}
\newlabel{eq:BC}{{15}{19}{Estimation error for Borda count algorithm}{equation.5.15}{}}
\MT@newlabel{eq:BC}
\newlabel{eq:rateBC}{{16}{19}{Estimation error for Borda count algorithm}{equation.5.16}{}}
\citation{wang2018learning,zhang2018tensor,kolda2009tensor}
\citation{wang2018learning}
\MT@newlabel{eq:rateBC}
\newlabel{eq:infinite}{{17}{20}{Sufficiently smooth tensors}{equation.5.17}{}}
\MT@newlabel{eq:infinite}
\MT@newlabel{eq:BC}
\@writefile{toc}{\contentsline {paragraph}{Hyperparameter tuning.}{20}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical analysis}{21}{section.6}\protected@file@percent }
\newlabel{sec:sim}{{6}{21}{Numerical analysis}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Synthetic data}{21}{subsection.6.1}\protected@file@percent }
\MT@newlabel{eq:rep}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Smooth functions in simulation. We define the numerical CP/Tucker rank as the minimal rank $r$ for which the relative approximation error is below $10^{-4}$. The reported rank in the table is estimated from a $100\times 100\times 100$ signal tensor generated by \MT_extended_eqref:n  {eq:rep}.\relax }}{21}{table.caption.9}\protected@file@percent }
\MT@newlabel{eq:rep}
\newlabel{tb:md}{{2}{21}{Smooth functions in simulation. We define the numerical CP/Tucker rank as the minimal rank $r$ for which the relative approximation error is below $10^{-4}$. The reported rank in the table is estimated from a $100\times 100\times 100$ signal tensor generated by \eqref {eq:rep}.\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Impacts of the number of blocks, tensor dimension, and polynomial degree.}{21}{section*.11}\protected@file@percent }
\citation{gao2015rate,klopp2017oracle}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces MSE versus the number of blocks based on different polynomial approximations. Columns 1-3 consider the Models 1, 3, and 5 respectively. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors.\relax }}{22}{figure.caption.10}\protected@file@percent }
\newlabel{fig:degk}{{3}{22}{MSE versus the number of blocks based on different polynomial approximations. Columns 1-3 consider the Models 1, 3, and 5 respectively. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors.\relax }{figure.caption.10}{}}
\citation{xu2018rates}
\citation{chatterjee2015matrix}
\citation{gao2015rate}
\citation{han2020exact}
\citation{balasubramanian2021nonparametric}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces MSE versus the tensor dimension based on different estimation methods. Columns 1-3 consider the Models 1, 3, and 5 in Table\nobreakspace  {}\ref  {tb:md} respectively. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors.\relax }}{23}{figure.caption.12}\protected@file@percent }
\newlabel{fig:method}{{4}{23}{MSE versus the tensor dimension based on different estimation methods. Columns 1-3 consider the Models 1, 3, and 5 in Table~\ref {tb:md} respectively. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison with alternative methods.}{23}{section*.14}\protected@file@percent }
\MT@newlabel{eq:lseopt}
\MT@newlabel{eq:lseopt}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance comparison among different methods. The observed data tensors, true signal tensors, and estimated signal tensors are plotted for Models 1, 3 and 5 in Table\nobreakspace  {}\ref  {tb:md} with fixed dimension $d = 80$. Numbers in parenthesis indicate the mean squared error.\relax }}{24}{figure.caption.13}\protected@file@percent }
\newlabel{fig:contim}{{5}{24}{Performance comparison among different methods. The observed data tensors, true signal tensors, and estimated signal tensors are plotted for Models 1, 3 and 5 in Table~\ref {tb:md} with fixed dimension $d = 80$. Numbers in parenthesis indicate the mean squared error.\relax }{figure.caption.13}{}}
\MT@newlabel{eq:lseopt}
\citation{Jeremy.2020}
\citation{Jeremy.2020}
\newlabel{sec:asym}{{6.1}{25}{Investigation of non-symmetric tensors}{section*.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Investigation of non-symmetric tensors.}{25}{section*.15}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces List of non-symmetric smooth functions in simulation.\relax }}{25}{table.caption.16}\protected@file@percent }
\newlabel{tb:md2}{{3}{25}{List of non-symmetric smooth functions in simulation.\relax }{table.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Hyperparameters for the methods under Models 1-5 in Table\nobreakspace  {}\ref  {tb:md2}. For {\bf  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Borda count} and {\bf  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip LSE} methods, the values in the table indicate the number of blocks. For {\bf  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Spectral} method, the first value indicates the tensor unfolding mode, while the second one represents the singular value threshold.\relax }}{25}{table.caption.17}\protected@file@percent }
\newlabel{tb:hyper}{{4}{25}{Hyperparameters for the methods under Models 1-5 in Table~\ref {tb:md2}. For {\bf \small Borda count} and {\bf \small LSE} methods, the values in the table indicate the number of blocks. For {\bf \small Spectral} method, the first value indicates the tensor unfolding mode, while the second one represents the singular value threshold.\relax }{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces MSEs from 20 repeated simulations based on different methods. All numbers are displayed on the scales $10^{-3}$. Standard errors are reported in parenthesis.\relax }}{26}{table.caption.18}\protected@file@percent }
\newlabel{tb:asymresult}{{5}{26}{MSEs from 20 repeated simulations based on different methods. All numbers are displayed on the scales $10^{-3}$. Standard errors are reported in parenthesis.\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Chicago crime maps. Figure(a) is the benchmark map based on homicides and shooting incidents in community areas in Chicago\nobreakspace  {}\citep  {Jeremy.2020}. Figure(b) shows the four clustered areas learned from 32 crime types using our method.\relax }}{26}{figure.caption.19}\protected@file@percent }
\newlabel{fig:area}{{6}{26}{Chicago crime maps. Figure(a) is the benchmark map based on homicides and shooting incidents in community areas in Chicago~\citep {Jeremy.2020}. Figure(b) shows the four clustered areas learned from 32 crime types using our method.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Applications to Chicago crime data}{26}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Averaged log counts of crimes according to crime types, hours, and the four areas estimated by our Borda count algorithm. We plot the estimated signal tensor entries averaged within four areas in the heatmap.\relax }}{27}{figure.caption.20}\protected@file@percent }
\newlabel{fig:crimeA}{{7}{27}{Averaged log counts of crimes according to crime types, hours, and the four areas estimated by our Borda count algorithm. We plot the estimated signal tensor entries averaged within four areas in the heatmap.\relax }{figure.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performance comparison in Chicago data analysis. Reported MSEs are averaged over five runs of cross-validation, with 20\% entries for testing and 80\% for training, with standard errors in parentheses. Block number is set to achieve the best prediction performance.\relax }}{28}{table.caption.21}\protected@file@percent }
\newlabel{tab:MSE}{{6}{28}{Performance comparison in Chicago data analysis. Reported MSEs are averaged over five runs of cross-validation, with 20\% entries for testing and 80\% for training, with standard errors in parentheses. Block number is set to achieve the best prediction performance.\relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and Discussions}{28}{section.7}\protected@file@percent }
\newlabel{sec:discussion}{{7}{28}{Conclusion and Discussions}{section.7}{}}
\citation{tibshirani2014adaptive,ortelli2019prediction}
\citation{chan2014consistent}
\citation{chan2014consistent,gao2015rate,klopp2017oracle,balasubramanian2021nonparametric}
\bibstyle{Chicago.bst}
\bibdata{tensor_wang}
\bibcite{balasubramanian2021nonparametric}{{1}{2021}{{Balasubramanian}}{{Balasubramanian}}}
\bibcite{baltrunas2011incarmusic}{{2}{2011}{{Baltrunas et~al.}}{{Baltrunas, Kaminskas, Ludwig, Moling, Ricci, Aydin, L{\"u}ke, and Schwaiger}}}
\bibcite{bi2018multilayer}{{3}{2018}{{Bi et~al.}}{{Bi, Qu, and Shen}}}
\bibcite{bickel2009nonparametric}{{4}{2009}{{Bickel and Chen}}{{Bickel and Chen}}}
\bibcite{chan2014consistent}{{5}{2014}{{Chan and Airoldi}}{{Chan and Airoldi}}}
\bibcite{chatterjee2015matrix}{{6}{2015}{{Chatterjee}}{{Chatterjee}}}
\bibcite{chen2021optimal}{{7}{2021}{{Chen et~al.}}{{Chen, Gao, and Zhang}}}
\bibcite{ding2021efficient}{{8}{2021}{{Ding et~al.}}{{Ding, Ma, Wu, and Xu}}}
\bibcite{flammarion2019optimal}{{9}{2019}{{Flammarion et~al.}}{{Flammarion, Mao, and Rigollet}}}
\bibcite{gao2015rate}{{10}{2015}{{Gao et~al.}}{{Gao, Lu, and Zhou}}}
\bibcite{gyorfi2002distribution}{{11}{2002}{{Gy{\"o}rfi et~al.}}{{Gy{\"o}rfi, Kohler, Krzy{\.z}ak, and Walk}}}
\bibcite{han2020exact}{{12}{2020}{{Han et~al.}}{{Han, Luo, Wang, and Zhang}}}
\bibcite{hore2016tensor}{{13}{2016}{{Hore et~al.}}{{Hore, Vi{\~n}uela, Buil, Knight, McCarthy, Small, and Marchini}}}
\bibcite{hutter2020estimation}{{14}{2020}{{H{\"u}tter et~al.}}{{H{\"u}tter, Mao, Rigollet, and Robeva}}}
\bibcite{Jeremy.2020}{{15}{2020}{{Jeremy}}{{Jeremy}}}
\bibcite{klopp2017oracle}{{16}{2017}{{Klopp et~al.}}{{Klopp, Tsybakov, and Verzelen}}}
\bibcite{kolda2009tensor}{{17}{2009}{{Kolda and Bader}}{{Kolda and Bader}}}
\bibcite{li2019nearest}{{18}{2019}{{Li et~al.}}{{Li, Shah, Song, and Yu}}}
\bibcite{livi2013graph}{{19}{2013}{{Livi and Rizzi}}{{Livi and Rizzi}}}
\bibcite{lovasz2012large}{{20}{2012}{{Lov{\'a}sz}}{{Lov{\'a}sz}}}
\bibcite{ortelli2019prediction}{{21}{2021}{{Ortelli and van~de Geer}}{{Ortelli and van~de Geer}}}
\bibcite{pananjady2020isotonic}{{22}{2021}{{Pananjady and Samworth}}{{Pananjady and Samworth}}}
\bibcite{phillippek2016}{{23}{2015}{{Phillippe~Rigollet}}{{Phillippe~Rigollet}}}
\bibcite{shah2019low}{{24}{2019}{{Shah et~al.}}{{Shah, Balakrishnan, and Wainwright}}}
\bibcite{stone1982optimal}{{25}{1982}{{Stone}}{{Stone}}}
\bibcite{sun2017provable}{{26}{2017}{{Sun et~al.}}{{Sun, Lu, Liu, and Cheng}}}
\bibcite{tibshirani2014adaptive}{{27}{2014}{{Tibshirani}}{{Tibshirani}}}
\bibcite{tsybakov2009introduction}{{28}{2009}{{Tsybakov}}{{Tsybakov}}}
\bibcite{wang2018learning}{{29}{2020}{{Wang and Li}}{{Wang and Li}}}
\bibcite{wang2019multiway}{{30}{2019}{{Wang and Zeng}}{{Wang and Zeng}}}
\bibcite{wasserman2006all}{{31}{2006}{{Wasserman}}{{Wasserman}}}
\bibcite{xu2018rates}{{32}{2018}{{Xu}}{{Xu}}}
\bibcite{young2018universality}{{33}{2018}{{Young et~al.}}{{Young, St-Onge, Desrosiers, and Dub{\'e}}}}
\bibcite{zhang2018tensor}{{34}{2018}{{Zhang and Xia}}{{Zhang and Xia}}}
\bibcite{zhao2015hypergraph}{{35}{2015}{{Zhao}}{{Zhao}}}
\bibcite{zhou2013tensor}{{36}{2013}{{Zhou et~al.}}{{Zhou, Li, and Zhu}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Extra numerical results}{34}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Results for Models 2 and 4 in Table\nobreakspace  {}\ref  {tab:comp}}{34}{subsection.A.1}\protected@file@percent }
\newlabel{sec:extra}{{A.1}{34}{Results for Models 2 and 4 in Table~\ref {tab:comp}}{subsection.A.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces MSE versus the tensor dimension based on different polynomial approximations. Columns 1-5 consider the Models 1-5 in Table\nobreakspace  {}\ref  {tb:md} respectively. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors.\relax }}{34}{figure.caption.24}\protected@file@percent }
\newlabel{fig:degdim}{{S1}{34}{MSE versus the tensor dimension based on different polynomial approximations. Columns 1-5 consider the Models 1-5 in Table~\ref {tb:md} respectively. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces Performance comparison between different methods. The observed data tensors, true signal tensors, and estimated signal tensors are plotted for Models 2 and 4 in Table\nobreakspace  {}\ref  {tb:md} with fixed dimension $d = 80$. Numbers in parenthesis indicate the mean squared error.\relax }}{35}{figure.caption.25}\protected@file@percent }
\newlabel{fig:extrav}{{S2}{35}{Performance comparison between different methods. The observed data tensors, true signal tensors, and estimated signal tensors are plotted for Models 2 and 4 in Table~\ref {tb:md} with fixed dimension $d = 80$. Numbers in parenthesis indicate the mean squared error.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S3}{\ignorespaces Simulation results for Models 2 and 4 in Table\nobreakspace  {}\ref  {tb:md}. Columns 1-2 plots MSE versus the number of blocks for different polynomial approximation, while Columns 3-4 shows the MSE versus the tensor dimension according to estimation methods. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors.\relax }}{35}{figure.caption.26}\protected@file@percent }
\newlabel{fig:extrasim1}{{S3}{35}{Simulation results for Models 2 and 4 in Table~\ref {tb:md}. Columns 1-2 plots MSE versus the number of blocks for different polynomial approximation, while Columns 3-4 shows the MSE versus the tensor dimension according to estimation methods. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Extra results on Chicago crime data analysis}{36}{subsection.A.2}\protected@file@percent }
\newlabel{subsec:chicago}{{A.2}{36}{Extra results on Chicago crime data analysis}{subsection.A.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {S1}{\ignorespaces Groups of crime types learned based on the Borda count estimation.\relax }}{36}{table.caption.27}\protected@file@percent }
\newlabel{tb:crimetb}{{S1}{36}{Groups of crime types learned based on the Borda count estimation.\relax }{table.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proofs of main theorems}{36}{appendix.B}\protected@file@percent }
\newlabel{app:theorem}{{B}{36}{Proofs of main theorems}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Proof of Lemma\nobreakspace  {}\ref  {lem:approx}}{36}{subsection.B.1}\protected@file@percent }
\newlabel{eq:ind}{{18}{36}{Proof of Lemma~\ref {lem:approx}}{equation.B.18}{}}
\newlabel{eq:polyapp}{{19}{37}{Proof of Lemma~\ref {lem:approx}}{equation.B.19}{}}
\MT@newlabel{eq:polyapp}
\MT@newlabel{eq:ind}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Proof of Theorem\nobreakspace  {}\ref  {thm:LSE}}{37}{subsection.B.2}\protected@file@percent }
\newlabel{sec:proofLSE}{{B.2}{37}{Proof of Theorem~\ref {thm:LSE}}{subsection.B.2}{}}
\newlabel{eq:approx}{{20}{37}{Proof of Theorem~\ref {thm:LSE}}{equation.B.20}{}}
\newlabel{eq:tri}{{21}{38}{Proof of Theorem~\ref {thm:LSE}}{equation.B.21}{}}
\newlabel{eq:embedding}{{22}{38}{Proof of Theorem~\ref {thm:LSE}}{equation.B.22}{}}
\newlabel{eq:union}{{23}{39}{Proof of Theorem~\ref {thm:LSE}}{equation.B.23}{}}
\MT@newlabel{eq:embedding}
\MT@newlabel{eq:union}
\newlabel{eq:union}{{24}{39}{Proof of Theorem~\ref {thm:LSE}}{equation.B.24}{}}
\MT@newlabel{eq:approx}
\MT@newlabel{eq:tri}
\MT@newlabel{eq:union}
\newlabel{eq:three}{{25}{39}{Proof of Theorem~\ref {thm:LSE}}{equation.B.25}{}}
\MT@newlabel{eq:three}
\MT@newlabel{eq:three}
\citation{gyorfi2002distribution}
\citation{stone1982optimal}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Proof of Theorem\nobreakspace  {}\ref  {thm:minimax}}{40}{subsection.B.3}\protected@file@percent }
\newlabel{eq:final}{{B.3}{40}{Proof of Theorem~\ref {thm:minimax}}{subsection.B.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Nonparametric rate.}{40}{section*.28}\protected@file@percent }
\newlabel{lem:non}{{3}{40}{Minimax rate for $\alpha $-smooth function estimation}{lem.3}{}}
\newlabel{eq:non}{{26}{41}{Nonparametric rate}{equation.B.26}{}}
\@writefile{toc}{\contentsline {paragraph}{Permutation rate.}{41}{section*.29}\protected@file@percent }
\newlabel{lem:permutation}{{4}{41}{Permutation error for tensor block model}{lem.4}{}}
\newlabel{eq:givenC}{{27}{41}{Permutation error for tensor block model}{equation.B.27}{}}
\MT@newlabel{eq:givenC}
\newlabel{eq:constructf}{{28}{42}{Permutation rate}{equation.B.28}{}}
\newlabel{eq:lower}{{29}{42}{Permutation rate}{equation.B.29}{}}
\MT@newlabel{eq:constructf}
\newlabel{eq:clusterrep}{{30}{42}{Permutation rate}{equation.B.30}{}}
\MT@newlabel{eq:lower}
\MT@newlabel{eq:clusterrep}
\newlabel{eq:permlower}{{31}{43}{Permutation rate}{equation.B.31}{}}
\@writefile{toc}{\contentsline {paragraph}{Combining two rates.}{43}{section*.30}\protected@file@percent }
\MT@newlabel{eq:non}
\MT@newlabel{eq:permlower}
\MT@newlabel{eq:rep}
\MT@newlabel{eq:minimax}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Proof of Lemma\nobreakspace  {}\ref  {lem:permute}}{43}{subsection.B.4}\protected@file@percent }
\newlabel{eq:concentration}{{32}{43}{Proof of Lemma~\ref {lem:permute}}{equation.B.32}{}}
\newlabel{eq:mon1}{{33}{43}{Proof of Lemma~\ref {lem:permute}}{equation.B.33}{}}
\newlabel{eq:mon2}{{34}{44}{Proof of Lemma~\ref {lem:permute}}{equation.B.34}{}}
\MT@newlabel{eq:mon1}
\MT@newlabel{eq:mon2}
\MT@newlabel{eq:mon1}
\MT@newlabel{eq:mon2}
\newlabel{eq:equiv}{{35}{44}{Proof of Lemma~\ref {lem:permute}}{equation.B.35}{}}
\MT@newlabel{eq:equiv}
\MT@newlabel{eq:concentration}
\MT@newlabel{eq:mon1}
\MT@newlabel{eq:mon2}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Proof of Theorem\nobreakspace  {}\ref  {thm:BC}}{45}{subsection.B.5}\protected@file@percent }
\MT@newlabel{eq:approx}
\newlabel{eq:3decomp}{{36}{45}{Proof of Theorem~\ref {thm:BC}}{equation.B.36}{}}
\newlabel{eq:permerror}{{37}{46}{Proof of Theorem~\ref {thm:BC}}{equation.B.37}{}}
\MT@newlabel{eq:permerror}
\newlabel{eq:embedding2}{{38}{46}{Proof of Theorem~\ref {thm:BC}}{equation.B.38}{}}
\MT@newlabel{eq:embedding2}
\newlabel{eq:nonparae}{{39}{46}{Proof of Theorem~\ref {thm:BC}}{equation.B.39}{}}
\MT@newlabel{eq:permerror}
\MT@newlabel{eq:nonparae}
\MT@newlabel{eq:3decomp}
\newlabel{eq:three2}{{40}{47}{Proof of Theorem~\ref {thm:BC}}{equation.B.40}{}}
\MT@newlabel{eq:three2}
\MT@newlabel{eq:three2}
\@writefile{toc}{\contentsline {section}{\numberline {C}Technical lemmas}{47}{appendix.C}\protected@file@percent }
\newlabel{sec:tech}{{C}{47}{Technical lemmas}{appendix.C}{}}
\newlabel{eq:structure}{{41}{47}{Technical lemmas}{equation.C.41}{}}
\citation{gao2015rate}
\newlabel{eq:sr}{{42}{48}{Technical lemmas}{equation.C.42}{}}
\newlabel{eq:kldistance}{{43}{48}{Technical lemmas}{equation.C.43}{}}
\MT@newlabel{eq:structure}
\newlabel{eq:packing}{{44}{48}{Technical lemmas}{equation.C.44}{}}
\MT@newlabel{eq:sr}
\newlabel{eq:n}{{45}{49}{Technical lemmas}{equation.C.45}{}}
\newlabel{eq:z}{{46}{49}{Technical lemmas}{equation.C.46}{}}
\MT@newlabel{eq:n}
\MT@newlabel{eq:z}
\MT@newlabel{eq:packing}
\newlabel{eq:fpacking}{{47}{49}{Technical lemmas}{equation.C.47}{}}
\MT@newlabel{eq:kldistance}
\MT@newlabel{eq:fpacking}
\MT@newlabel{eq:structure}
\newlabel{lem:embedding}{{5}{49}{Sub-Gaussian maxima under full embedding}{lem.5}{}}
\citation{phillippek2016}
\newlabel{lem:subga}{{6}{50}{Theorem 1.19 in \citet {phillippek2016}}{lem.6}{}}
\citation{gao2015rate}
\newlabel{prop:minmax}{{1}{51}{Proposition 4.1 in \citet {gao2015rate}}{prop.1}{}}
\newlabel{lem:covering}{{7}{51}{Varshamov-Gilbert bound}{lem.7}{}}
